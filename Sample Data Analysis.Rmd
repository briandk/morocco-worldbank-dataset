---
title: "Analyzing a Sample Dataset from the World Bank"
author: "Brian Danielak"
date: "April 12, 2015"
output:
  html_document:
    toc: true
    theme: default
  rmarkdown::tufte_handout:
    toc: true
---

# Introduction

Before we get into analyzing this data, we'll need to do some housekeeping first. In what follows, we'll:

1. Load some non-base R packages we'll be using
2. Load the CSV data into R
3. Check whether the data looks reasonable
4. Perform some data cleaning

## Loading Extra Packages for this Analysis

```{r, message=FALSE}
library(httr)
library(jsonlite)
library(magrittr)
library(readr)
library(dplyr)
library(xtable)
library(knitr)
library(stringr)
```



## Getting the Data from the WorldBank

I opted to access the data by [exporting a CSV file][1] from the World Bank's site. 

```{r}
load_data <- function () {
  return (
    read.csv(
      file = "WorldBankData.csv",
      header = TRUE,
      stringsAsFactors = FALSE
    ) %>% 
      tbl_df()
  )
}

world_bank_data <- load_data()
```

## Checking the Reasonability of the Data

### What the data provider says the data should look like

From the [World Bank's website][2]:

> This dataset contains raw response data to a nano-survey that was conducted in Morocco on the right to access to information and the right to petition and motion the government. A nano-survey is an innovative technology that extends a brief survey to a random sampling of internet users. Note: "NA" or "N/A" indicates "No Answer." Sub-national location data is available for 2/3 of survey recipients. 54,441 random internet users in Morocco were exposed to a portion of this survey, with 15,020 respondents providing at least partial responses.

### Checking that the Data Match the Provider's Claims

We should make sure our data looks OK after all that loading. Sometimes anything from a misplaced comma to a poorly-terminated line can really junk up the works, so let's check by trying to inspect the first few rows.

```{r}
head(world_bank_data)[ ,1:4] %>% 
  kable
```

Good. The tabular structure came through alright. What about the overall structure of the data?

```{r}
str(world_bank_data)
```

Also good! We've showing the same 54,441 cases noted by the data provider. 

## Preliminary Data Cleaning

We know from the provider's description that some NAs will be encoded as either `NA` or `N/A`, and it might be wise to replace those character values with R's built-in `NA` value.

As an example, case 123 has some NA values in it:

```{r}
world_bank_data[123, ] %>% 
  select(
    country.region,
    country.region_code
  )
```

So, let's use `apply()` to replace those character values with built-in `NA` values. 

```{r}
clean_na_values <- function (data_to_clean) {
  na_accumulator <- NULL
  replace_na_strings_with_na_values <- function (x) {
    if (x %in% c("NA", "N/A")) {
      x <- NA
      print("Found an NA!")
      na_accumulator <- c(na_accumulator, TRUE)
    }
    return (x)
  }
  data_to_clean %<>%
    apply(
      X = .,
      MARGIN = c(1, 2),
      FUN = replace_na_strings_with_na_values
    ) %>% 
    as.data.frame() %>% 
    tbl_df()
  print(
    length(na_accumulator)
  )
  return (data_to_clean)
}

# world_bank_data %<>% clean_na_values()
```

If our code worked, we should be able to verify that R now recognizes NA values.

```{r}
# All values should be TRUE, indicating they are NAs
world_bank_data[123, ] %>% 
  select(
    country.region,
    country.region_code
  ) %>% 
  is.na()
```

## A Note on Data Cleaning Performance

Using `apply()` here might not be the most performant option for doing NA replacement speedily. So, if speed is a crucial concern (and in some setups it can be), there are two non-mutually-exclusive options we might consider as first-order solutions for refactoring this code:

1. Exploring whether optimized functions in the `stringr` library (such as `str_match` and `str_replace`) or other text libraries would make faster replacements in some of the NA-replacement logic.
2. Perform this computation once, up front, then cache the result. At the very least, we could accomplish this up-front computation/caching pattern by running the existing `apply()` code and saving the resulting dataframe as a `.Rda` file. In future runs of this code, we can then check whether that `.Rda` file exists, and if for some reason it doesn't, only then would we re-run the `apply()` code that generates it.

But, there's also a third option: check whether our `apply()` code made any replacements in the first place.

After poking around my NA-cleaning code, I started to wonder whether it was actually making any substitutions. So, I cobbled together a bit of a test. In the code of my `clean_na_values` function, I included an `na_accumulator`, which the inner function has access to via closure. If my replacement logic ever takes the `if` branch and actually replaces a text `"NA"` or `"N/A"` value, we append a `TRUE` to that vector. When the `apply()` function finishes, we can then check the length of the `na_accumulator` to see how many replacements the `apply` section made. In this case, the answer is zero. None.

I could have just had a basic print statement print "NA substitution" each time, but I didn't want to risk littering the screen with all the possible NAs.

So, assuming my code is working properly, this test seems to confirm that instead of replacing the matching logic or using caching, the best refactor might be to remove the `apply` section entirely. It's not making a single replacement, which means case 123 was untouched:

```{r}
world_bank_data[123, ] %>% 
  select(
    country.region,
    country.region_code
  ) %>% 
  is.na()
```



```{r}
complete.cases(world_bank_data)[122:124]
```





[1]: https://finances.worldbank.org/api/views/tg37-mj88/rows.csv?accessType=DOWNLOAD
[2]: https://finances.worldbank.org/dataset/World-Bank-Morocco-Citizen-Engagement-Nano-Survey-/tg37-mj88


