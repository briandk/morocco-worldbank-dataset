---
title: "Analyzing a Sample Dataset from the World Bank"
author: "Brian Danielak"
date: "April 12, 2015"
output:
  html_document:
    toc: true
    theme: default
  rmarkdown::tufte_handout:
    toc: true
---

# Overview

In what follows, I'll lead you through a very preliminary look at the [World Bank Morocco Citizen Engagement Nano-Survey][2]. Here's a roadmap of where we'll go and what we'll do

1. Load some non-base R packages we'll be using
2. Load the CSV data into R
3. Check whether the data looks reasonable
4. Perform some data cleaning
5. Note some performance issues with the data cleaning and explore how we can improve them
6. Inspect and visualize the data for complete cases
7. Generate some questions to guide future work

## Loading Extra Packages for this Analysis

```{r, message=FALSE}
library(httr)
library(magrittr)
library(readr)
library(dplyr)
library(knitr)
library(ggplot2)
```



## Getting the Data from the WorldBank

I opted to access the data by [exporting a CSV file][1] from the World Bank's site. 

```{r}
load_data <- function () {
  return (
    read.csv(
      file = "WorldBankData.csv",
      header = TRUE,
      stringsAsFactors = FALSE
    ) %>% 
      tbl_df()
  )
}

world_bank_data <- load_data()
```

## Checking the Reasonability of the Data

### What the data provider says the data should look like

From the [World Bank's website][2]:

> This dataset contains raw response data to a nano-survey that was conducted in Morocco on the right to access to information and the right to petition and motion the government. A nano-survey is an innovative technology that extends a brief survey to a random sampling of internet users. Note: "NA" or "N/A" indicates "No Answer." Sub-national location data is available for 2/3 of survey recipients. 54,441 random internet users in Morocco were exposed to a portion of this survey, with 15,020 respondents providing at least partial responses.

### Checking that the Data Match the Provider's Claims

We should make sure our data looks OK after all that loading. Sometimes anything from a misplaced comma to a poorly-terminated line can really junk up the works, so let's check by trying to inspect the first few rows.

```{r}
head(world_bank_data)[ ,1:4] %>% 
  kable
```

Good. The tabular structure came through alright. What about the overall structure of the data?

```{r}
str(world_bank_data)
```

Also good! We've showing the same 54,441 cases noted by the data provider. 

## Preliminary Data Cleaning

We know from the provider's description that some NAs will be encoded as either `NA` or `N/A`, and it might be wise to replace those character values with R's built-in `NA` value.

As an example, case 123 has some NA values in it:

```{r}
world_bank_data[123, ] %>% 
  select(
    country.region,
    country.region_code
  )
```

So, let's use `apply()` to replace those character values with built-in `NA` values. 

```{r}
clean_na_values <- function (data_to_clean) {
  na_accumulator <- NULL
  replace_na_strings_with_na_values <- function (x) {
    if (x %in% c("NA", "N/A")) {
      x <- NA
      print("Found an NA!")
      na_accumulator <- c(na_accumulator, TRUE)
    }
    return (x)
  }
  data_to_clean %<>%
    apply(
      X = .,
      MARGIN = c(1, 2),
      FUN = replace_na_strings_with_na_values
    ) %>% 
    as.data.frame() %>% 
    tbl_df()
  print(
    length(na_accumulator)
  )
  return (data_to_clean)
}

# world_bank_data %<>% clean_na_values()
```

If our code worked, we should be able to verify that R now recognizes NA values.

```{r}
# All values should be TRUE, indicating they are NAs
world_bank_data[123, ] %>% 
  select(
    country.region,
    country.region_code
  ) %>% 
  is.na()
```

## A Note on Data Cleaning Performance

Using `apply()` here might not be the most performant option for doing NA replacement speedily. So, if speed is a crucial concern (and in some setups it can be), there are two non-mutually-exclusive options we might consider as first-order solutions for refactoring this code:

1. Exploring whether optimized functions in the `stringr` library (such as `str_match` and `str_replace`) or other text libraries would make faster replacements in some of the NA-replacement logic.
2. Perform this computation once, up front, then cache the result. At the very least, we could accomplish this up-front computation/caching pattern by running the existing `apply()` code and saving the resulting dataframe as a `.Rda` file. In future runs of this code, we can then check whether that `.Rda` file exists, and if for some reason it doesn't, only then would we re-run the `apply()` code that generates it.

But, there's also a third option: check whether our `apply()` code made any replacements in the first place.

After poking around my NA-cleaning code, I started to wonder whether it was actually making any substitutions. So, I cobbled together a bit of a test. In the code of my `clean_na_values` function, I included an `na_accumulator`, which the inner function has access to via closure. If my replacement logic ever takes the `if` branch and actually replaces a text `"NA"` or `"N/A"` value, we append a `TRUE` to that vector. When the `apply()` function finishes, we can then check the length of the `na_accumulator` to see how many replacements the `apply` section made. In this case, the answer is zero. None.

I could have just had a basic print statement print "NA substitution" each time, but I didn't want to risk littering the screen with all the possible NAs.

So, assuming my code is working properly, this test seems to confirm that instead of replacing the matching logic or using caching, the best refactor might be to remove the `apply` section entirely. It's not making a single replacement, which means case 123 was untouched:

```{r}
world_bank_data[123, ] %>% 
  select(
    country.region,
    country.region_code
  ) %>% 
  is.na()
```

## Inspecting the Data for Complete Cases

We know there's a high degree of missingness in the data, so it seems reasonable to ask how many observations are free of `NA`s.

```{r}
world_bank_data %>% 
  complete.cases() %>% 
  which()
```

Hmm. That's actually...curious. Hang on a sec, let's visualize those complete cases again.

```{r}
get_data_for_complete_cases_plot <- function (input_data) {
  return(
    world_bank_data %>% 
      mutate(is_complete_case = complete.cases(input_data)) %>% 
      mutate(case_id = 1:dim(input_data)[1])
  )
}

p <- ggplot(
  aes(
    x = is_complete_case,
    y = case_id
  ),
  data = get_data_for_complete_cases_plot(world_bank_data)
)
p <- p + geom_point()
print(p)
```

Now, it may seem a bit silly to have done this visualization, but I would argue it's actually useful, if unusual. This visualization confirms, in one glance, several things:

1. **Complete cases comprise fewer than 1% of all the data here.** Yes, we could have calculated that just from the length of the `complete.cases()` output, but this visualization gives us a clear reminder of just how small that fraction is. Actually, the faction is even smaller than what we see, because ggplot2's default point size is so fat with respect to the scales of this graphic that the graphic visually overstates the fraction of complete cases.
2. **All the complete cases cluster in the first lines of the dataset**. We couldn't have known that from the author's provided description of the dataset. We might have discovered it by digging through the text output of `complete.cases()`, 




[1]: https://finances.worldbank.org/api/views/tg37-mj88/rows.csv?accessType=DOWNLOAD
[2]: https://finances.worldbank.org/dataset/World-Bank-Morocco-Citizen-Engagement-Nano-Survey-/tg37-mj88


